{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "quant-01-general.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swha815/Paper_List-GPGPU/blob/master/quant_01_general.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBqnwmHyE9yk"
      },
      "source": [
        "<p><img alt=\"Colaboratory logo\" height=\"45px\" src=\"/img/colab_favicon.ico\" align=\"left\" hspace=\"10px\" vspace=\"0px\"></p>\n",
        "\n",
        "<H1>Neural Network Quantization</H1>\n",
        "\n",
        "Porting floating-point (single-precision) numbers to integers (16 bits or less)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwsRvr83Iyqf"
      },
      "source": [
        "## I. Characteristics\n",
        "\n",
        "### Advantages\n",
        "\n",
        "- Smaller memory footprint\n",
        "- Faster computation (roughly 5X)\n",
        "\n",
        "### Disadvantages\n",
        "\n",
        "- Loss of critical information when not done appropriately\n",
        "- Extra burden of value conversion\n",
        "  - Weights\n",
        "    - Inference: can be pre-processed and used without modification during run-time\n",
        "    - Training: must be quantized with every update (not true for Kahan summation-based methods)\n",
        "  - Activation: input to NN must be quantized and output de-quantized "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_DFfcc9dNl-"
      },
      "source": [
        "## II. Qauntifying Quantization\n",
        "\n",
        "### MSE\n",
        "\n",
        "Mean-squared-error (MSE) is commonly used to measure the difference between two signals due to its simplicity.\n",
        "\n",
        "\\begin{equation}\n",
        "MSE = \\frac{1}{n} \\sum \\limits^n (x - \\hat{x})^2\n",
        "\\tag{1}\n",
        "\\end{equation}\n",
        "\n",
        "A major pitfall of using MSE for quantifying the loss arising from quantization is that it does not consider the overall magnitude of the values, thereby hindering direct comparison with other layers/channels.\n",
        "\n",
        "### PSNR\n",
        "Peak-signal-to-noise-ratio (PSNR) is another popular metric.\n",
        "\n",
        "\\begin{equation}\n",
        "PSNR = 20 \\times log_{10} (\\hat{x}_{max}) - 10 \\times log_{10} (MSE)\n",
        "\\tag{2}\n",
        "\\end{equation}\n",
        "\n",
        "Unfortunately, PSNR can be deceivingly high when quantization range is overestimated.\n",
        "\n",
        "### SQNR\n",
        "\n",
        "Tranditional signal-to-quantization-noise ratio (SQNR) measure is defined as follows:\n",
        "\n",
        "\\begin{equation}\n",
        "SQNR = \\frac{P_{signal}}{P_{noise}} = \\frac{E[x^2]}{E[\\tilde{x}^2]} = \\frac{\\int x^2f(x)\\mathrm{d}x}{\\frac{x^2_{max}}{3\\times4^v}} = \\frac{3 \\times 4^v \\times {\\int x^2f(x)\\mathrm{d}x}}{x^2_{max}}\n",
        "\\tag{3}\n",
        "\\end{equation}\n",
        "\n",
        "Assuming quantization noise is defined as additive noise which is uniformly distributed and that quantization is symmetric and linear, it can be approximated as the following:\n",
        "\n",
        "\\begin{equation}\n",
        "P_{noise} = \\int^{-m}_{-\\infty} x^2 f(x) \\mathrm{d}x + \\int^n_{-m} (x-\\hat{x})^2 f(x) \\mathrm{d}x + \\int^\\infty_n x^2 f(x) \\mathrm{d}x \\\\ \\hspace{-45pt} = 2 \\int^\\infty_m x^2 f(x) \\mathrm{d}x + \\int^m_{-m} (x-\\hat{x})^2 f(x)\\mathrm{d}x\n",
        "\\tag{4}\n",
        "\\end{equation}\n",
        "\n",
        "where, $m = \\Delta \\times 2^{b-1}$\n",
        "\n",
        "Once an appropriate distribution can be determined, minimum value for $P_{noise}$ can be determined given a specific bit-width.\n",
        "\n",
        "SQNR is a nice measurement in that, unlike MSE, it can be used directly amongst layers/channels in a neural network to compare the level of deterioration from quantization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctBZqBDXJQP8"
      },
      "source": [
        "## III. Typical Process of NN Quantization\n",
        "\n",
        "### Prerequisite\n",
        "\n",
        "1. Review the target HW architecture\n",
        "1. Identify where MSB/LSB truncation error (round-off and clamping) occurs\n",
        "1. Discard negligible truncation error\n",
        "1. Consider techniques to minimize truncation error (i.e. BatchNorm folding)\n",
        "\n",
        "### Weights\n",
        "\n",
        "1. **Profile** and collect statistics from kernel\n",
        "1. **Analyze** gathered information\n",
        "1. **Decide** quantization strategy\n",
        "  - Granularity: layer-wise, output channel-wise, full channel-wise, etc\n",
        "  - Symmetry: symmetric or asymmetric\n",
        "  - Step-Size: uniform or non-uniform\n",
        "1. **Simulate** HW kernel store by replacing original weights with quantized weights\n",
        "  - Inference-only: quantize during pre-process stage\n",
        "  - Training: quantize after every update\n",
        "\n",
        "### Activations\n",
        "\n",
        "1. **Profile** and collect statistics from where truncation error is likely to occur (i.e. output feature map)\n",
        "1. **Analyze** gathered information and try to find/fit an appropriate distribution\n",
        "1. **Decide** quantization strategy (dependent on HW architecture)\n",
        "  - Granularity: layer-wise, channel-wise, etc\n",
        "  - Symmetry: symmetric or asymmetric\n",
        "  - Step-Size: uniform (linear) or non-uniform (quadratic or LUT-based)\n",
        "1. **Simulate** integer-based HW with _fake_ quantization layer"
      ]
    }
  ]
}