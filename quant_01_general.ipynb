{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "quant-01-general.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swha815/Paper_List-GPGPU/blob/master/quant_01_general.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBqnwmHyE9yk"
      },
      "source": [
        "<p><img alt=\"Colaboratory logo\" height=\"45px\" src=\"/img/colab_favicon.ico\" align=\"left\" hspace=\"10px\" vspace=\"0px\"></p>\n",
        "\n",
        "<H1>Neural Network Quantization</H1>\n",
        "\n",
        "Porting floating-point (single-precision) numbers to integers (16 bits or less)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gwsRvr83Iyqf"
      },
      "source": [
        "## I. Characteristics\n",
        "\n",
        "### Advantages\n",
        "\n",
        "- Smaller memory footprint\n",
        "- Faster computation (roughly 5X)\n",
        "\n",
        "### Disadvantages\n",
        "\n",
        "- Loss of critical information when not done properly\n",
        "- Extra burden of value conversion\n",
        "  - Weights\n",
        "    - Inference: can be pre-processed and used without modification during run-time\n",
        "    - Training: must be quantized with every update (not true for Kahan summation-based methods)\n",
        "  - Activation: input to NN must be quantized and output de-quantized "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2JgG2P2ZVWGj"
      },
      "source": [
        "## II. Taxonomy of Methods\n",
        "\n",
        "### Symmetry\n",
        "\n",
        "#### Symmetric\n",
        "\n",
        "Distribution of the target numbers are assumed to have a mean centered around zero with $-m \\le x \\le m$.\n",
        "\n",
        "![Symmetric](https://raw.githubusercontent.com/swha815/colab/main/img/symmetric.jpg)\n",
        "\n",
        "#### Unsigned Symmetric\n",
        "\n",
        "Activations after ReLU will always be $0 \\le x$. Therefore, the bit assigned to negative values can be used to make positive values to twice densely covered with discrete integer (quantized) representations.\n",
        "\n",
        "![Symmetric-unsigned](https://raw.githubusercontent.com/swha815/colab/main/img/symmetric-unsigned.jpg)\n",
        "\n",
        "#### Asymmetric\n",
        "\n",
        "Histogram of layer parameters almost always show that the kernel values are never centered around zero. Hence, asymmetric quantization better represents these values. However, asymmetric weight quantization scheme requires partial sums of feature maps to be calculated during run-time, requring extra HW or increasing latency.\n",
        "\n",
        "![Asymmetric](https://raw.githubusercontent.com/swha815/colab/main/img/asymmetric.jpg)\n",
        "\n",
        "### Linearity\n",
        "\n",
        "#### Linear\n",
        "\n",
        "In this most common scheme, equally spaced discrete values are assigned to the numbers to be represented. Quantization and de-quantization can be achieved by simply multiplying by a scale factor. \n",
        "\n",
        "#### Non-Linear, but Continuous\n",
        "\n",
        "Schemes such as exponential/log quantization assigns discrete values with gradually increasing/decreasing spacing between discrete values. As most of the values to be represented are centered near zero, assigning more bits is a straight-forward way to minimize quantization noise. However, quantization/dequantization requires power function calculation.\n",
        "\n",
        "#### Look-up Table\n",
        "\n",
        "This method has the highest possibility to minimize the quantization noise. However, scatter/gather operation is required prior to dot-product calculation.\n",
        "\n",
        "### Granularity\n",
        "\n",
        "#### Fixed-Point\n",
        "\n",
        "A single shared exponent exists across the whole network. Calculation between channels and layers are consistent, therefore, is the simplest to implement in HW.\n",
        "\n",
        "#### Dynamic Fixed-Point\n",
        "\n",
        "Each channel or layer have distinct shared exponent which is meaningful only within that channel/layer. In channel-wise quantization, dot-product will incur addition of products with varying exponents, therefore, requires special consideration (simplest solution is to employ shifters immediately before the addition).\n",
        "\n",
        "#### Block Fixed-Point\n",
        "\n",
        "A pre-defined set of numbers are assigned to a shared exponent usually correlated with the HW's computation unit. Appending shifters to the inputs of the multiplier or accumulator is usually required.\n",
        "\n",
        "#### Low-Precision Floating-Point\n",
        "\n",
        "Each value faciliates its own dedicated exponent. Computation requires floating-point computation units with lower precision than FP32. However, limited exponent bits can cause severe accuracy degradation from overflow/underflow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_DFfcc9dNl-"
      },
      "source": [
        "## III. Qauntifying Effect of Quantization\n",
        "\n",
        "### MSE\n",
        "\n",
        "Mean-squared-error (MSE) is commonly used to measure the difference between two signals due to its simplicity.\n",
        "\n",
        "\\begin{equation}\n",
        "MSE = \\frac{1}{n} \\sum \\limits^n (x - \\hat{x})^2\n",
        "\\tag{1}\n",
        "\\end{equation}\n",
        "\n",
        "A major pitfall of using MSE for quantifying the loss arising from quantization is that it does not consider the overall magnitude of the values, thereby hindering direct comparison with other layers/channels.\n",
        "\n",
        "### PSNR\n",
        "Peak-signal-to-noise-ratio (PSNR) is another popular metric.\n",
        "\n",
        "\\begin{equation}\n",
        "PSNR = 20 \\times log_{10} (\\hat{x}_{max}) - 10 \\times log_{10} (MSE)\n",
        "\\tag{2}\n",
        "\\end{equation}\n",
        "\n",
        "Unfortunately, PSNR can be deceivingly high when quantization range is overestimated.\n",
        "\n",
        "### SQNR\n",
        "\n",
        "Tranditional signal-to-quantization-noise ratio (SQNR) measure is defined as follows:\n",
        "\n",
        "\\begin{equation}\n",
        "SQNR = \\frac{P_{signal}}{P_{noise}} = \\frac{E[x^2]}{E[\\tilde{x}^2]} = \\frac{\\int x^2f(x)\\mathrm{d}x}{\\frac{x^2_{max}}{3\\times4^v}} = \\frac{3 \\times 4^v \\times {\\int x^2f(x)\\mathrm{d}x}}{x^2_{max}}\n",
        "\\tag{3}\n",
        "\\end{equation}\n",
        "\n",
        "Assuming quantization noise is defined as additive noise which is uniformly distributed and that quantization is symmetric and linear, it can be approximated as the following:\n",
        "\n",
        "\\begin{equation}\n",
        "P_{noise} = \\int^{-m}_{-\\infty} x^2 f(x) \\mathrm{d}x + \\int^n_{-m} (x-\\hat{x})^2 f(x) \\mathrm{d}x + \\int^\\infty_n x^2 f(x) \\mathrm{d}x \\\\ \\hspace{-45pt} = 2 \\int^\\infty_m x^2 f(x) \\mathrm{d}x + \\int^m_{-m} (x-\\hat{x})^2 f(x)\\mathrm{d}x\n",
        "\\tag{4}\n",
        "\\end{equation}\n",
        "\n",
        "where, $m = \\Delta \\times 2^{b-1}$\n",
        "\n",
        "Once an appropriate distribution can be found, minimum value for $P_{noise}$ can be determined given a specific bit-width.\n",
        "\n",
        "SQNR is a nice measurement in that, unlike MSE, it can be used directly amongst layers/channels in a neural network to compare the level of deterioration from quantization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ctBZqBDXJQP8"
      },
      "source": [
        "## IV. Typical Process of NN Quantization\n",
        "\n",
        "### Prerequisite\n",
        "\n",
        "1. Review the target HW architecture\n",
        "1. Identify where MSB/LSB truncation error (round-off and clamping) occurs\n",
        "1. Discard negligible truncation error\n",
        "1. Consider techniques to minimize truncation error (i.e. BatchNorm folding)\n",
        "\n",
        "### Weights\n",
        "\n",
        "1. **Profile** and collect statistics from kernel\n",
        "1. **Analyze** gathered information\n",
        "1. **Decide** quantization strategy\n",
        "  - Granularity: layer-wise, output channel-wise, full channel-wise, etc\n",
        "  - Symmetry: symmetric or asymmetric\n",
        "  - Step-Size: uniform or non-uniform\n",
        "1. **Simulate** HW kernel store by replacing original weights with quantized weights\n",
        "  - Inference-only: quantize during pre-process stage\n",
        "  - Training: quantize after every update\n",
        "\n",
        "### Activations\n",
        "\n",
        "1. **Profile** and collect statistics from where truncation error is likely to occur (i.e. output feature map)\n",
        "1. **Analyze** gathered information and try to find/fit an appropriate distribution\n",
        "1. **Decide** quantization strategy (dependent on HW architecture)\n",
        "  - Granularity: layer-wise, channel-wise, etc\n",
        "  - Symmetry: symmetric or asymmetric\n",
        "  - Step-Size: uniform (linear) or non-uniform (quadratic or LUT-based)\n",
        "1. **Simulate** integer-based HW with _fake_ quantization layer"
      ]
    }
  ]
}